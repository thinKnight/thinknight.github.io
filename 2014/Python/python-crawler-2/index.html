<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="description" content="I will survive"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="alternative" href="/atom.xml" title="Matrix Wall" type="application/atom+xml"><link rel="icon" href="/favicon.png"><title>Python网络爬虫（二） - Matrix Wall</title><link rel="stylesheet" href="/css/main.css"><!--[if lt IE 9]><script>(function(a,b){a="abbr article aside audio bdi canvas data datalist details dialog figcaption figure footer header hgroup main mark meter nav output progress section summary template time video".split(" ");for(b=a.length-1;b>=0;b--)document.createElement(a[b])})()</script><![endif]--><link rel="stylesheet" href="/css/prism.css" type="text/css"><script src="/js/prism.js"></script></head><body><header class="head"><h1 class="head-title u-fl"><a href="/">Matrix Wall</a></h1><nav class="head-nav u-fr"><ul class="head-nav__list"><li class="head-nav__item"><a href="/" class="head-nav__link">Home</a></li><li class="head-nav__item"><a href="/archives" class="head-nav__link">Archives</a></li></ul></nav></header><main class="main"><article class="post"><header class="post__head"><time datetime="2014-10-23T10:51:36.000Z" class="post__time">October 23, 2014</time><h1 class="post__title"><a href="/2014/Python/python-crawler-2/">Python网络爬虫（二）</a></h1></header><div class="post__main echo"><p>上次写到了<strong>遍历网站的全部页面</strong>和<strong>向百度提交搜索</strong>，但是其中还存在着许多的问题。<br><a id="more"></a></p>
<h2 id="解析HTML"><a href="#解析HTML" class="headerlink" title="解析HTML"></a>解析HTML</h2><p>在上次的方法中，由于以前都是用简单的正则表达式来解析HTML，所以为了尝鲜我就使用了BeautifulSoup和SGMLParser两种方法。但是经过使用下来发现还是BeautifulSoup好那么一点，而且官方的<a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html#" target="_blank" rel="noopener">文档</a>也很详实，以后用起来也会更加方便。  </p>
<p>当然了，这两者在解析的过程中都有自己的局限性，所以还得配合正则表达式使用。  </p>
<h2 id="循环与递归"><a href="#循环与递归" class="headerlink" title="循环与递归"></a>循环与递归</h2><p>由于上次处理的只是遍历一个页面的URL，所以总的来说工作量比较小，然后我就用了<strong>递归</strong>这种最笨的方法。  </p>
<p>但是显而易见，递归是一个非常耗内存的差方法，用递归写过输出斐波那契数列的人都知道，从第十几个数字后就开始慢的不行了，而且最近还听说某厂面试一个应届生的时候因为他用递归处理斐波那契就直接拒了他……所以还是不用的好。  </p>
<p>由于Python中自带队列数据结构，所以通过队列实现迭代循环是目前较为理想的方案。  </p>
<h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><p>在爬虫程序中，当我提交了请求之后CPU需要等待网站相应后才能进一步计算，也就是需要等待<code>urllib2.urlopen()</code>得到相应之后才能read网页的内容，所以这就需要等待一段时间，所以为了提高爬虫的效率，就需要开启多线程进程抓取。</p>
<h2 id="健壮"><a href="#健壮" class="headerlink" title="健壮"></a>健壮</h2><p>在爬虫运行的时候，如果因为被网站的防爬虫机制禁止了爬取行为，那就会导致整个爬虫程序的意外退出，所以就必须把<code>urllib2</code>的行为包起来。  </p>
<p>另外，如果同一个IP在短时间内对一个网站进行大量访问，可能会被网站的防爬虫措施制裁，比如豆瓣…所以为了避免爬虫挂掉，就得设置一个时间间隔，也就是让线程暂时阻塞，等时间到了之后再加入线程队列中。  </p>
<h2 id="Bloom-Filter"><a href="#Bloom-Filter" class="headerlink" title="Bloom Filter"></a>Bloom Filter</h2><p>在上次的遍历一个URL中的所有URL任务中，虽然一次能抓取到几千个URL，但是并不能保证这些URL都是不重复的，如果在这些URL中有环路的话，爬虫就会先入死循环中，所以对抓取到的URL进行去重就是一个要面临的问题。  </p>
<p>当需要处理的数据很少的时候，可以用<code>set</code>集合来解决，但是当数据量变大的时候，就得靠<strong>Bloom Filter</strong>（布隆过滤器）了。BF的算法不算非常复杂，不过好歹有现成的<a href="https://github.com/jaybaird/python-bloomfilter/blob/master/pybloom/pybloom.py" target="_blank" rel="noopener">轮子</a>，用起来也方便了许多。  </p>
<h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> pybloom <span class="keyword">import</span> BloomFilter</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = BloomFilter(capacity=<span class="number">10000</span>, error_rate=<span class="number">0.001</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i <span class="keyword">in</span> range_fn(<span class="number">0</span>, f.capacity):</span><br><span class="line"><span class="meta">... </span>_ = f.add(i)</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">0</span> <span class="keyword">in</span> f</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.capacity <span class="keyword">in</span> f</span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(f) &lt;= f.capacity</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>(<span class="number">1.0</span> - (len(f) / float(f.capacity))) &lt;= f.error_rate + <span class="number">2e-18</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> pybloom <span class="keyword">import</span> ScalableBloomFilter</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sbf = ScalableBloomFilter(mode=ScalableBloomFilter.SMALL_SET_GROWTH)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>count = <span class="number">10000</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i <span class="keyword">in</span> range_fn(<span class="number">0</span>, count):</span><br><span class="line"><span class="meta">... </span>_ = sbf.add(i)</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sbf.capacity &gt; count</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(sbf) &lt;= count</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>(<span class="number">1.0</span> - (len(sbf) / float(count))) &lt;= sbf.error_rate + <span class="number">2e-18</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line">------------</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新任务</span></span><br><span class="line">&gt;自动向百度提交搜索请求，搜索nuist.edu.cn中包含？的URL，从返回的结果页面中，提取每一个分页中的URL，并将结果写入一个文件中。**这次强调所有结果有多少页就爬取多少页**！</span><br><span class="line"></span><br><span class="line"><span class="comment">## 实现</span></span><br><span class="line">```Python</span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> Queue</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">import</span> threading </span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> pybloom <span class="keyword">import</span> BloomFilter</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># use Bloom Filter</span></span><br><span class="line">bf = BloomFilter(<span class="number">1000000</span>, <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># translate the default code</span></span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(<span class="string">"utf-8"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define a queue</span></span><br><span class="line">url_wait = Queue.Queue(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread</span><span class="params">(threading.Thread)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, url, num)</span>:</span></span><br><span class="line">        threading.Thread.__init__(self)</span><br><span class="line">        self.url = url</span><br><span class="line">        <span class="comment"># self.tnum = num</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># traverse the whole url</span></span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">        traverse(self.url)</span><br><span class="line">        <span class="comment"># print "This is thread-%d" % self.tnum</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_nextpage</span><span class="params">(new_url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        tmp = urllib2.urlopen(new_url)</span><br><span class="line">        content = tmp.read()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    soup = BeautifulSoup(content)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> soup.find_all(href=re.compile(<span class="string">"rsv_page=1"</span>)):</span><br><span class="line">        tmp_link = link.get(<span class="string">'href'</span>)</span><br><span class="line">        real_url = <span class="string">"http://www.baidu.com"</span> + tmp_link</span><br><span class="line">        <span class="keyword">return</span> real_url  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">traverse</span><span class="params">(url)</span>:</span></span><br><span class="line">    </span><br><span class="line">    fp = open(<span class="string">"all_url.txt"</span>, <span class="string">"a"</span>)</span><br><span class="line"></span><br><span class="line">    url_wait.put(url)</span><br><span class="line">     </span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> url_wait.empty():</span><br><span class="line">        url = url_wait.get()</span><br><span class="line">        <span class="keyword">if</span> url <span class="keyword">not</span> <span class="keyword">in</span> bf:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                content = urllib2.urlopen(url).read() </span><br><span class="line">                soup = BeautifulSoup(content)                                     </span><br><span class="line">                <span class="keyword">for</span> urls <span class="keyword">in</span> soup.find_all(href=re.compile(<span class="string">"http"</span>)):                     </span><br><span class="line">                    link = urls.get(<span class="string">'href'</span>)</span><br><span class="line">                    url_wait.put(link)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">            bf.add(url)</span><br><span class="line">            fp.write( url + <span class="string">'\n\n'</span>)   </span><br><span class="line"></span><br><span class="line">    fp.close()  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line">    fp = open(<span class="string">"target.txt"</span>, <span class="string">"a"</span>)</span><br><span class="line">    url_pool = Queue.Queue(<span class="number">0</span>)</span><br><span class="line">    start_url = <span class="string">"http://www.baidu.com/s?wd=site:(nuist.edu.cn)%20?"</span></span><br><span class="line">    </span><br><span class="line">    url_pool.put(start_url)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> url_pool.empty():</span><br><span class="line">            new_url = url_pool.get()</span><br><span class="line">            fp.write(new_url + <span class="string">"\n\n"</span>)</span><br><span class="line"></span><br><span class="line">            nextpage = find_nextpage(new_url)</span><br><span class="line">            url_pool.put(nextpage)</span><br><span class="line">        </span><br><span class="line">            Thread = MyThread(new_url, num)</span><br><span class="line">            num += <span class="number">1</span></span><br><span class="line">            Thread.start()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    fp.close()  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:  </span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
</div><footer class="post__foot u-cf"><ul class="post__tag u-fl"><li class="post__tag__item"><a href="/tags/Python/" class="post__tag__link">Python</a></li></ul><a href="/2014/Python/python-crawler-2/#disqus_thread" class="post__foot-link u-fr">0 COMMENTS</a></footer></article><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript></div></div></main><footer class="foot"><div class="foot-copy u-fl">&copy; 2019 thinKnight</div><menu class="page-menu u-fr"><li class="page-menu__item"><a title="Previous" href="/2015/Tool/vim/" class="page-menu__link icon-arrow-left"></a></li><li class="page-menu__item"><a title="Next" href="/2014/Compiler/Lexical-analyzer/" class="page-menu__link icon-arrow-right"></a></li></menu></footer><script>(function(h,g,l,k,j,i){j=h.createElement(g),i=h.getElementsByTagName(g)[0],
j.src="//"+l+".disqus.com/"+k+".js",i.parentNode.insertBefore(j,i)})
(document,"script","matrixwall","embed");
</script></body></html>