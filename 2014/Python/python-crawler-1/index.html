<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="description" content="I will survive"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="alternative" href="/atom.xml" title="Matrix Wall" type="application/atom+xml"><link rel="icon" href="/favicon.png"><title>Python网络爬虫（一） - Matrix Wall</title><link rel="stylesheet" href="/css/main.css"><!--[if lt IE 9]><script>(function(a,b){a="abbr article aside audio bdi canvas data datalist details dialog figcaption figure footer header hgroup main mark meter nav output progress section summary template time video".split(" ");for(b=a.length-1;b>=0;b--)document.createElement(a[b])})()</script><![endif]--><link rel="stylesheet" href="/css/prism.css" type="text/css"><script src="/js/prism.js"></script></head><body><header class="head"><h1 class="head-title u-fl"><a href="/">Matrix Wall</a></h1><nav class="head-nav u-fr"><ul class="head-nav__list"><li class="head-nav__item"><a href="/" class="head-nav__link">Home</a></li><li class="head-nav__item"><a href="/archives" class="head-nav__link">Archives</a></li></ul></nav></header><main class="main"><article class="post"><header class="post__head"><time datetime="2014-10-13T13:48:33.000Z" class="post__time">October 13, 2014</time><h1 class="post__title"><a href="/2014/Python/python-crawler-1/">Python网络爬虫（一）</a></h1></header><div class="post__main echo"><p>从零开始写爬虫<br><a id="more"></a></p>
<h2 id="一、遍历网站的全部页面"><a href="#一、遍历网站的全部页面" class="headerlink" title="一、遍历网站的全部页面"></a>一、遍历网站的全部页面</h2><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>要遍历一个网站的全部页面,要做的就是先打开目标网站的源码,从中提取所有的URL,然后再逐个遍历,并保存已处理过的URL。  </p>
<h4 id="1-提取URL"><a href="#1-提取URL" class="headerlink" title="1.提取URL"></a>1.提取URL</h4><p>从一堆HTML中提取可用的URL是一件轻松的事,处理的方法也有很多。  </p>
<ul>
<li>正则表达式提取  </li>
<li>BeautifulSoup提取</li>
<li>自带库sgmlib中的SGMLParser类  </li>
</ul>
<p>这里就试试第三种方法。  </p>
<h4 id="2-存储URL"><a href="#2-存储URL" class="headerlink" title="2.存储URL"></a>2.存储URL</h4><p>提取出URL后,分开存储刚刚提取出来的URL和已经处理过的URL就是接下来要解决的问题。  </p>
<p>一开始想过存在列表里面,但是从中提取和pop出URL的顺序又成了问题,所以这里采用Python自带的<strong>队列</strong>数据结构。  </p>
<p>然后处理完的URL就直接存入文件,并且计数,即为已经访问到的页面数量。  </p>
<h4 id="3-实现函数"><a href="#3-实现函数" class="headerlink" title="3.实现函数"></a>3.实现函数</h4><p>接下来的任务就是运行函数来处理URL了,但是我现在用的只是最笨的<strong>递归</strong>,效率低下不说,对内存也是一个很大的考验。所以之后会通过多线程编程来解决这个问题,把URL放入内存池中,规定每次允许运行的线程数,这样就能在一定程度上提升效率和速度了。  </p>
<h4 id="4-具体代码"><a href="#4-具体代码" class="headerlink" title="4.具体代码"></a>4.具体代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="comment"># 搜寻现成的爬虫代码，弄明白怎样遍历一个网站的全部页面，编码实现：</span></span><br><span class="line"><span class="comment"># 能够遍历一个网站的大部分页面，保存输出可遍历页面的URL，并统计访问到的页面数量。</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> Queue</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">from</span> sgmllib <span class="keyword">import</span> SGMLParser</span><br><span class="line"></span><br><span class="line">url_queue = Queue.Queue(<span class="number">0</span>)</span><br><span class="line">url_num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">find_url</span><span class="params">(SGMLParser)</span>:</span></span><br><span class="line">	<span class="string">"""docstring for find_url</span></span><br><span class="line"><span class="string">		store the urls into url_new</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">	"""</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		SGMLParser.__init__(self)</span><br><span class="line">		self.url_new = []</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">start_a</span><span class="params">(self, attrs)</span>:</span></span><br><span class="line">		href = [v <span class="keyword">for</span> k, v <span class="keyword">in</span> attrs <span class="keyword">if</span> k==<span class="string">'href'</span>] </span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> re.match(<span class="string">r'^https?:/&#123;2&#125;\w.+$'</span>, <span class="string">""</span>.join(href)):</span><br><span class="line">			self.url_new.extend(href)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">open_url</span><span class="params">()</span>:</span></span><br><span class="line">	url = url + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">	url_given = url_queue.get()</span><br><span class="line"></span><br><span class="line">	url_traversed.write(url_given + <span class="string">"\n"</span>) </span><br><span class="line"></span><br><span class="line">	content = urllib2.urlopen(url_given).read()	</span><br><span class="line">	result  = find_url()</span><br><span class="line">	result.feed(content) </span><br><span class="line">	<span class="keyword">for</span> urls <span class="keyword">in</span> result.url_new:</span><br><span class="line">		<span class="comment"># print i                 </span></span><br><span class="line">		url_queue.put(urls)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">while</span> <span class="keyword">not</span> url_queue.empty():</span><br><span class="line">		open_url()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">	url = <span class="string">"http://movie.douban.com"</span></span><br><span class="line"></span><br><span class="line">	url_traversed = open(<span class="string">'URLSTORE.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line">	url_queue.put(url)</span><br><span class="line"></span><br><span class="line">	open_url()</span><br><span class="line"></span><br><span class="line">	url_traversed.closed()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">print</span> <span class="string">"The number of the traversed URL is %d"</span> % url_num</span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line"><span class="comment">## 二、向百度提交搜索</span></span><br><span class="line"><span class="comment">#### 1.提交搜索</span></span><br><span class="line">我现在还是用的最笨的方法,即直接打开包含需要搜索内容的URL,就能得到搜索页面的源码。  </span><br><span class="line"></span><br><span class="line">用POST和GET提交的方法下次再用。  </span><br><span class="line"></span><br><span class="line"><span class="comment">#### 2.处理结果  </span></span><br><span class="line">用BeautifulSoup查找&lt;div&gt;标签间的内容,但是这个还是只能大概地过滤,并不能很精准地返回搜索内容。  </span><br><span class="line"></span><br><span class="line"><span class="comment">#### 3.待解问题</span></span><br><span class="line"><span class="number">1.</span> 用POST和GET方法提交搜索。  </span><br><span class="line"><span class="number">2.</span> 细致地处理返回的搜索结果。  </span><br><span class="line"><span class="number">3.</span> 遍历所有的搜索结果。  </span><br><span class="line"></span><br><span class="line">```Python</span><br><span class="line"><span class="comment"># -*-coding: UTF-8 -*-</span></span><br><span class="line"><span class="comment"># 2.用百度设置内的高级搜索功能，在指定网站中搜索URL中包含？的结果。编程实现：</span></span><br><span class="line"><span class="comment"># 	自动向百度提交搜索，在指在指定网站中搜索URL中包含？的结果，提取百度搜索结果并输出到文件。</span></span><br><span class="line"><span class="comment">#   例如，搜索nuist.edu.cn，其实就是想百度提交搜索字符串site:(nuist.edu.cn) ?</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># http://www.baidu.com/s?wd=</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">reload(sys)   </span><br><span class="line">sys.setdefaultencoding(<span class="string">'utf8'</span>)  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search_baidu</span><span class="params">()</span>:</span></span><br><span class="line">    url = urllib2.urlopen(<span class="string">"http://www.baidu.com/s?wd=site:(nuist.edu.cn)%20?"</span>)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">    urltmp = url.read()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># urltmp = urltmp.decode("UTF-8").encode("UTF-8")</span></span><br><span class="line"></span><br><span class="line">    soup = BeautifulSoup(urltmp)</span><br><span class="line"></span><br><span class="line">    res = soup.find(name=<span class="string">'div'</span>).getText(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    ss = open(<span class="string">'ss.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line">    ss.write(res)</span><br><span class="line">    ss.close()</span><br><span class="line"></span><br><span class="line">search_baidu()</span><br></pre></td></tr></table></figure>
</div><footer class="post__foot u-cf"><ul class="post__tag u-fl"><li class="post__tag__item"><a href="/tags/Python/" class="post__tag__link">Python</a></li></ul><a href="/2014/Python/python-crawler-1/#disqus_thread" class="post__foot-link u-fr">0 COMMENTS</a></footer></article><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript></div></div></main><footer class="foot"><div class="foot-copy u-fl">&copy; 2019 thinKnight</div><menu class="page-menu u-fr"><li class="page-menu__item"><a title="Previous" href="/2014/Compiler/Lexical-analyzer/" class="page-menu__link icon-arrow-left"></a></li><li class="page-menu__item"><a title="Next" href="/2014/OS/process-qa/" class="page-menu__link icon-arrow-right"></a></li></menu></footer><script>(function(h,g,l,k,j,i){j=h.createElement(g),i=h.getElementsByTagName(g)[0],
j.src="//"+l+".disqus.com/"+k+".js",i.parentNode.insertBefore(j,i)})
(document,"script","matrixwall","embed");
</script></body></html>