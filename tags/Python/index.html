<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="description" content="I will survive"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="alternative" href="/atom.xml" title="Matrix Wall" type="application/atom+xml"><link rel="icon" href="/favicon.png"><title>Python - Matrix Wall</title><link rel="stylesheet" href="/css/main.css"><!--[if lt IE 9]><script>(function(a,b){a="abbr article aside audio bdi canvas data datalist details dialog figcaption figure footer header hgroup main mark meter nav output progress section summary template time video".split(" ");for(b=a.length-1;b>=0;b--)document.createElement(a[b])})()</script><![endif]--><link rel="stylesheet" href="/css/prism.css" type="text/css"><script src="/js/prism.js"></script></head><body><header class="head"><h1 class="head-title u-fl"><a href="/">Matrix Wall</a></h1><nav class="head-nav u-fr"><ul class="head-nav__list"><li class="head-nav__item"><a href="/" class="head-nav__link">Home</a></li><li class="head-nav__item"><a href="/archives" class="head-nav__link">Archives</a></li></ul></nav></header><main class="main"><article class="post"><header class="post__head"><time datetime="2014-10-23T10:51:36.000Z" class="post__time">October 23, 2014</time><h1 class="post__title"><a href="/2014/Python/python-crawler-2/">Python网络爬虫（二）</a></h1></header><div class="post__main echo"><p>上次写到了<strong>遍历网站的全部页面</strong>和<strong>向百度提交搜索</strong>，但是其中还存在着许多的问题。<br><a id="more"></a></p>
<h2 id="解析HTML"><a href="#解析HTML" class="headerlink" title="解析HTML"></a>解析HTML</h2><p>在上次的方法中，由于以前都是用简单的正则表达式来解析HTML，所以为了尝鲜我就使用了BeautifulSoup和SGMLParser两种方法。但是经过使用下来发现还是BeautifulSoup好那么一点，而且官方的<a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html#" target="_blank" rel="noopener">文档</a>也很详实，以后用起来也会更加方便。  </p>
<p>当然了，这两者在解析的过程中都有自己的局限性，所以还得配合正则表达式使用。  </p>
<h2 id="循环与递归"><a href="#循环与递归" class="headerlink" title="循环与递归"></a>循环与递归</h2><p>由于上次处理的只是遍历一个页面的URL，所以总的来说工作量比较小，然后我就用了<strong>递归</strong>这种最笨的方法。  </p>
<p>但是显而易见，递归是一个非常耗内存的差方法，用递归写过输出斐波那契数列的人都知道，从第十几个数字后就开始慢的不行了，而且最近还听说某厂面试一个应届生的时候因为他用递归处理斐波那契就直接拒了他……所以还是不用的好。  </p>
<p>由于Python中自带队列数据结构，所以通过队列实现迭代循环是目前较为理想的方案。  </p>
<h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><p>在爬虫程序中，当我提交了请求之后CPU需要等待网站相应后才能进一步计算，也就是需要等待<code>urllib2.urlopen()</code>得到相应之后才能read网页的内容，所以这就需要等待一段时间，所以为了提高爬虫的效率，就需要开启多线程进程抓取。</p>
<h2 id="健壮"><a href="#健壮" class="headerlink" title="健壮"></a>健壮</h2><p>在爬虫运行的时候，如果因为被网站的防爬虫机制禁止了爬取行为，那就会导致整个爬虫程序的意外退出，所以就必须把<code>urllib2</code>的行为包起来。  </p>
<p>另外，如果同一个IP在短时间内对一个网站进行大量访问，可能会被网站的防爬虫措施制裁，比如豆瓣…所以为了避免爬虫挂掉，就得设置一个时间间隔，也就是让线程暂时阻塞，等时间到了之后再加入线程队列中。  </p>
<h2 id="Bloom-Filter"><a href="#Bloom-Filter" class="headerlink" title="Bloom Filter"></a>Bloom Filter</h2><p>在上次的遍历一个URL中的所有URL任务中，虽然一次能抓取到几千个URL，但是并不能保证这些URL都是不重复的，如果在这些URL中有环路的话，爬虫就会先入死循环中，所以对抓取到的URL进行去重就是一个要面临的问题。  </p>
<p>当需要处理的数据很少的时候，可以用<code>set</code>集合来解决，但是当数据量变大的时候，就得靠<strong>Bloom Filter</strong>（布隆过滤器）了。BF的算法不算非常复杂，不过好歹有现成的<a href="https://github.com/jaybaird/python-bloomfilter/blob/master/pybloom/pybloom.py" target="_blank" rel="noopener">轮子</a>，用起来也方便了许多。  </p>
<h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> pybloom <span class="keyword">import</span> BloomFilter</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = BloomFilter(capacity=<span class="number">10000</span>, error_rate=<span class="number">0.001</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i <span class="keyword">in</span> range_fn(<span class="number">0</span>, f.capacity):</span><br><span class="line"><span class="meta">... </span>_ = f.add(i)</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">0</span> <span class="keyword">in</span> f</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.capacity <span class="keyword">in</span> f</span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(f) &lt;= f.capacity</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>(<span class="number">1.0</span> - (len(f) / float(f.capacity))) &lt;= f.error_rate + <span class="number">2e-18</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> pybloom <span class="keyword">import</span> ScalableBloomFilter</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sbf = ScalableBloomFilter(mode=ScalableBloomFilter.SMALL_SET_GROWTH)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>count = <span class="number">10000</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i <span class="keyword">in</span> range_fn(<span class="number">0</span>, count):</span><br><span class="line"><span class="meta">... </span>_ = sbf.add(i)</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sbf.capacity &gt; count</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(sbf) &lt;= count</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>(<span class="number">1.0</span> - (len(sbf) / float(count))) &lt;= sbf.error_rate + <span class="number">2e-18</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line">------------</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新任务</span></span><br><span class="line">&gt;自动向百度提交搜索请求，搜索nuist.edu.cn中包含？的URL，从返回的结果页面中，提取每一个分页中的URL，并将结果写入一个文件中。**这次强调所有结果有多少页就爬取多少页**！</span><br><span class="line"></span><br><span class="line"><span class="comment">## 实现</span></span><br><span class="line">```Python</span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> Queue</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">import</span> threading </span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> pybloom <span class="keyword">import</span> BloomFilter</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># use Bloom Filter</span></span><br><span class="line">bf = BloomFilter(<span class="number">1000000</span>, <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># translate the default code</span></span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(<span class="string">"utf-8"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define a queue</span></span><br><span class="line">url_wait = Queue.Queue(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread</span><span class="params">(threading.Thread)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, url, num)</span>:</span></span><br><span class="line">        threading.Thread.__init__(self)</span><br><span class="line">        self.url = url</span><br><span class="line">        <span class="comment"># self.tnum = num</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># traverse the whole url</span></span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">        traverse(self.url)</span><br><span class="line">        <span class="comment"># print "This is thread-%d" % self.tnum</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_nextpage</span><span class="params">(new_url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        tmp = urllib2.urlopen(new_url)</span><br><span class="line">        content = tmp.read()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    soup = BeautifulSoup(content)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> soup.find_all(href=re.compile(<span class="string">"rsv_page=1"</span>)):</span><br><span class="line">        tmp_link = link.get(<span class="string">'href'</span>)</span><br><span class="line">        real_url = <span class="string">"http://www.baidu.com"</span> + tmp_link</span><br><span class="line">        <span class="keyword">return</span> real_url  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">traverse</span><span class="params">(url)</span>:</span></span><br><span class="line">    </span><br><span class="line">    fp = open(<span class="string">"all_url.txt"</span>, <span class="string">"a"</span>)</span><br><span class="line"></span><br><span class="line">    url_wait.put(url)</span><br><span class="line">     </span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> url_wait.empty():</span><br><span class="line">        url = url_wait.get()</span><br><span class="line">        <span class="keyword">if</span> url <span class="keyword">not</span> <span class="keyword">in</span> bf:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                content = urllib2.urlopen(url).read() </span><br><span class="line">                soup = BeautifulSoup(content)                                     </span><br><span class="line">                <span class="keyword">for</span> urls <span class="keyword">in</span> soup.find_all(href=re.compile(<span class="string">"http"</span>)):                     </span><br><span class="line">                    link = urls.get(<span class="string">'href'</span>)</span><br><span class="line">                    url_wait.put(link)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">            bf.add(url)</span><br><span class="line">            fp.write( url + <span class="string">'\n\n'</span>)   </span><br><span class="line"></span><br><span class="line">    fp.close()  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line">    fp = open(<span class="string">"target.txt"</span>, <span class="string">"a"</span>)</span><br><span class="line">    url_pool = Queue.Queue(<span class="number">0</span>)</span><br><span class="line">    start_url = <span class="string">"http://www.baidu.com/s?wd=site:(nuist.edu.cn)%20?"</span></span><br><span class="line">    </span><br><span class="line">    url_pool.put(start_url)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> url_pool.empty():</span><br><span class="line">            new_url = url_pool.get()</span><br><span class="line">            fp.write(new_url + <span class="string">"\n\n"</span>)</span><br><span class="line"></span><br><span class="line">            nextpage = find_nextpage(new_url)</span><br><span class="line">            url_pool.put(nextpage)</span><br><span class="line">        </span><br><span class="line">            Thread = MyThread(new_url, num)</span><br><span class="line">            num += <span class="number">1</span></span><br><span class="line">            Thread.start()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    fp.close()  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:  </span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
</div><footer class="post__foot u-cf"><ul class="post__tag u-fl"><li class="post__tag__item"><a href="/tags/Python/" class="post__tag__link">Python</a></li></ul><a href="/2014/Python/python-crawler-2/#disqus_thread" class="post__foot-link u-fr">0 COMMENTS</a></footer></article><article class="post"><header class="post__head"><time datetime="2014-10-13T13:48:33.000Z" class="post__time">October 13, 2014</time><h1 class="post__title"><a href="/2014/Python/python-crawler-1/">Python网络爬虫（一）</a></h1></header><div class="post__main echo"><p>从零开始写爬虫<br><a id="more"></a></p>
<h2 id="一、遍历网站的全部页面"><a href="#一、遍历网站的全部页面" class="headerlink" title="一、遍历网站的全部页面"></a>一、遍历网站的全部页面</h2><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>要遍历一个网站的全部页面,要做的就是先打开目标网站的源码,从中提取所有的URL,然后再逐个遍历,并保存已处理过的URL。  </p>
<h4 id="1-提取URL"><a href="#1-提取URL" class="headerlink" title="1.提取URL"></a>1.提取URL</h4><p>从一堆HTML中提取可用的URL是一件轻松的事,处理的方法也有很多。  </p>
<ul>
<li>正则表达式提取  </li>
<li>BeautifulSoup提取</li>
<li>自带库sgmlib中的SGMLParser类  </li>
</ul>
<p>这里就试试第三种方法。  </p>
<h4 id="2-存储URL"><a href="#2-存储URL" class="headerlink" title="2.存储URL"></a>2.存储URL</h4><p>提取出URL后,分开存储刚刚提取出来的URL和已经处理过的URL就是接下来要解决的问题。  </p>
<p>一开始想过存在列表里面,但是从中提取和pop出URL的顺序又成了问题,所以这里采用Python自带的<strong>队列</strong>数据结构。  </p>
<p>然后处理完的URL就直接存入文件,并且计数,即为已经访问到的页面数量。  </p>
<h4 id="3-实现函数"><a href="#3-实现函数" class="headerlink" title="3.实现函数"></a>3.实现函数</h4><p>接下来的任务就是运行函数来处理URL了,但是我现在用的只是最笨的<strong>递归</strong>,效率低下不说,对内存也是一个很大的考验。所以之后会通过多线程编程来解决这个问题,把URL放入内存池中,规定每次允许运行的线程数,这样就能在一定程度上提升效率和速度了。  </p>
<h4 id="4-具体代码"><a href="#4-具体代码" class="headerlink" title="4.具体代码"></a>4.具体代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="comment"># 搜寻现成的爬虫代码，弄明白怎样遍历一个网站的全部页面，编码实现：</span></span><br><span class="line"><span class="comment"># 能够遍历一个网站的大部分页面，保存输出可遍历页面的URL，并统计访问到的页面数量。</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> Queue</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">from</span> sgmllib <span class="keyword">import</span> SGMLParser</span><br><span class="line"></span><br><span class="line">url_queue = Queue.Queue(<span class="number">0</span>)</span><br><span class="line">url_num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">find_url</span><span class="params">(SGMLParser)</span>:</span></span><br><span class="line">	<span class="string">"""docstring for find_url</span></span><br><span class="line"><span class="string">		store the urls into url_new</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">	"""</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">		SGMLParser.__init__(self)</span><br><span class="line">		self.url_new = []</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">start_a</span><span class="params">(self, attrs)</span>:</span></span><br><span class="line">		href = [v <span class="keyword">for</span> k, v <span class="keyword">in</span> attrs <span class="keyword">if</span> k==<span class="string">'href'</span>] </span><br><span class="line">		</span><br><span class="line">		<span class="keyword">if</span> re.match(<span class="string">r'^https?:/&#123;2&#125;\w.+$'</span>, <span class="string">""</span>.join(href)):</span><br><span class="line">			self.url_new.extend(href)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">open_url</span><span class="params">()</span>:</span></span><br><span class="line">	url = url + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">	url_given = url_queue.get()</span><br><span class="line"></span><br><span class="line">	url_traversed.write(url_given + <span class="string">"\n"</span>) </span><br><span class="line"></span><br><span class="line">	content = urllib2.urlopen(url_given).read()	</span><br><span class="line">	result  = find_url()</span><br><span class="line">	result.feed(content) </span><br><span class="line">	<span class="keyword">for</span> urls <span class="keyword">in</span> result.url_new:</span><br><span class="line">		<span class="comment"># print i                 </span></span><br><span class="line">		url_queue.put(urls)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">while</span> <span class="keyword">not</span> url_queue.empty():</span><br><span class="line">		open_url()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">	url = <span class="string">"http://movie.douban.com"</span></span><br><span class="line"></span><br><span class="line">	url_traversed = open(<span class="string">'URLSTORE.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line">	url_queue.put(url)</span><br><span class="line"></span><br><span class="line">	open_url()</span><br><span class="line"></span><br><span class="line">	url_traversed.closed()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">print</span> <span class="string">"The number of the traversed URL is %d"</span> % url_num</span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line"><span class="comment">## 二、向百度提交搜索</span></span><br><span class="line"><span class="comment">#### 1.提交搜索</span></span><br><span class="line">我现在还是用的最笨的方法,即直接打开包含需要搜索内容的URL,就能得到搜索页面的源码。  </span><br><span class="line"></span><br><span class="line">用POST和GET提交的方法下次再用。  </span><br><span class="line"></span><br><span class="line"><span class="comment">#### 2.处理结果  </span></span><br><span class="line">用BeautifulSoup查找&lt;div&gt;标签间的内容,但是这个还是只能大概地过滤,并不能很精准地返回搜索内容。  </span><br><span class="line"></span><br><span class="line"><span class="comment">#### 3.待解问题</span></span><br><span class="line"><span class="number">1.</span> 用POST和GET方法提交搜索。  </span><br><span class="line"><span class="number">2.</span> 细致地处理返回的搜索结果。  </span><br><span class="line"><span class="number">3.</span> 遍历所有的搜索结果。  </span><br><span class="line"></span><br><span class="line">```Python</span><br><span class="line"><span class="comment"># -*-coding: UTF-8 -*-</span></span><br><span class="line"><span class="comment"># 2.用百度设置内的高级搜索功能，在指定网站中搜索URL中包含？的结果。编程实现：</span></span><br><span class="line"><span class="comment"># 	自动向百度提交搜索，在指在指定网站中搜索URL中包含？的结果，提取百度搜索结果并输出到文件。</span></span><br><span class="line"><span class="comment">#   例如，搜索nuist.edu.cn，其实就是想百度提交搜索字符串site:(nuist.edu.cn) ?</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># http://www.baidu.com/s?wd=</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">reload(sys)   </span><br><span class="line">sys.setdefaultencoding(<span class="string">'utf8'</span>)  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search_baidu</span><span class="params">()</span>:</span></span><br><span class="line">    url = urllib2.urlopen(<span class="string">"http://www.baidu.com/s?wd=site:(nuist.edu.cn)%20?"</span>)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">    urltmp = url.read()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># urltmp = urltmp.decode("UTF-8").encode("UTF-8")</span></span><br><span class="line"></span><br><span class="line">    soup = BeautifulSoup(urltmp)</span><br><span class="line"></span><br><span class="line">    res = soup.find(name=<span class="string">'div'</span>).getText(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    ss = open(<span class="string">'ss.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line">    ss.write(res)</span><br><span class="line">    ss.close()</span><br><span class="line"></span><br><span class="line">search_baidu()</span><br></pre></td></tr></table></figure>
</div><footer class="post__foot u-cf"><ul class="post__tag u-fl"><li class="post__tag__item"><a href="/tags/Python/" class="post__tag__link">Python</a></li></ul><a href="/2014/Python/python-crawler-1/#disqus_thread" class="post__foot-link u-fr">0 COMMENTS</a></footer></article><article class="post"><header class="post__head"><time datetime="2014-08-28T07:08:40.000Z" class="post__time">August 28, 2014</time><h1 class="post__title"><a href="/2014/Python/douban-crawler/">豆瓣爬虫</a></h1></header><div class="post__main echo"><p>可抓取豆瓣读书、电影、音乐中任意标签下内容<br><a id="more"></a></p>
<p>在我刚刚入门Python爬虫的时候，无论怎样都很难找到一个适当的实例让我参考。  </p>
<p>看过很多别人的例子，但都觉得不得要领，所以在折腾很久后写了这个简单的例子。  </p>
<hr>
<h3 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="comment"># 如果要在python2的py文件里面写中文，则必须要添加一行声明文件编码的注释，否则python2会默认使用ASCII编码。  </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re </span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">douban_crawler</span><span class="params">(url_head, target)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">1000</span>, <span class="number">20</span>):</span><br><span class="line">    <span class="comment">#这个1000是检索的条目数量，可以按需设定</span></span><br><span class="line">        url_rear = <span class="string">"?start=%d&amp;type=T"</span> % page</span><br><span class="line">        url_use = url_head + url_rear</span><br><span class="line">        <span class="comment">#两段合成真正的url</span></span><br><span class="line">        content = urllib2.urlopen(url_use).read()</span><br><span class="line">        content = content.decode(<span class="string">"UTF-8"</span>).encode(<span class="string">"UTF-8"</span>)</span><br><span class="line">        </span><br><span class="line">        content = content.replace(<span class="string">r'title="去FM收听"'</span>, <span class="string">""</span>)</span><br><span class="line">        content = content.replace(<span class="string">r'title="去其他标签"'</span>, <span class="string">""</span>)</span><br><span class="line">        </span><br><span class="line">        name = re.findall(<span class="string">r'title="(\S*?)"'</span>, content, re.S)</span><br><span class="line">        <span class="comment">#正则表达式捕获标题</span></span><br><span class="line">        num  = re.findall(<span class="string">r'&lt;span\s*class="rating_nums"&gt;([0-9.]*)&lt;\/span&gt;'</span>, content)</span><br><span class="line">        <span class="comment">#正则表达式捕获分数</span></span><br><span class="line">        </span><br><span class="line">        doc = zip(name, num)</span><br><span class="line">        <span class="comment">#将标题和分数打包成([ , ][ , ]...)的形式</span></span><br><span class="line">        <span class="keyword">if</span> target == <span class="string">"book"</span>:</span><br><span class="line">            dou = open(<span class="string">"doc_book.txt"</span>, <span class="string">'a'</span>)</span><br><span class="line">        <span class="keyword">elif</span> target == <span class="string">"music"</span>:</span><br><span class="line">            dou = open(<span class="string">"doc_music.txt"</span>, <span class="string">'a'</span>)</span><br><span class="line">        <span class="keyword">elif</span> target == <span class="string">"movie"</span>:</span><br><span class="line">            dou = open(<span class="string">"doc_movie.txt"</span>, <span class="string">'a'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> doc:</span><br><span class="line">            dou.write(i[<span class="number">0</span>] + <span class="string">" "</span> + i[<span class="number">1</span>] + <span class="string">"\n"</span>)</span><br><span class="line">            <span class="comment">#写入</span></span><br><span class="line">    dou.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    target = raw_input(<span class="string">"豆瓣 book movie music，你想爬哪一个? "</span>)</span><br><span class="line"></span><br><span class="line">    tag   = raw_input(<span class="string">"请输入你想要检索的标签: "</span>)</span><br><span class="line"></span><br><span class="line">    url_head  = <span class="string">"http://%s.douban.com/tag/%s"</span> % (target, tag)</span><br><span class="line"></span><br><span class="line">    douban_crawler(url_head, target)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">print</span> <span class="string">"抓取完毕"</span></span><br></pre></td></tr></table></figure>
<h3 id="抓取结果"><a href="#抓取结果" class="headerlink" title="抓取结果"></a>抓取结果</h3><ul>
<li><strong>豆瓣读书-小说</strong></li>
</ul>
<blockquote>
<p>月亮和六便士 9.0<br>百年孤独 9.2<br>解忧杂货店 8.7<br>追风筝的人 8.8<br>霍乱时期的爱情 9.0<br>平凡的世界（全三部） 9.0<br>围城 8.9<br>活着 9.1<br>一九八四 9.3<br>人生的枷锁 9.0<br>陆犯焉识 8.7<br>…  </p>
</blockquote>
<ul>
<li><strong>豆瓣电影-悬疑</strong></li>
</ul>
<blockquote>
<p>盗梦空间 9.2<br>寒战 7.4<br>嫌疑人X的献身 7.4<br>七宗罪 8.7<br>致命ID 8.5<br>云图 8.0<br>禁闭岛 8.5<br>蝴蝶效应 8.6<br>致命魔术 8.8<br>恐怖游轮 8.2<br>…</p>
</blockquote>
<ul>
<li><strong>豆瓣音乐-pop</strong></li>
</ul>
<blockquote>
<p>十二新作 8.2<br>Alright,Still 7.7<br>范特西 8.5<br>Apologize 8.9<br>逆光 7.3<br>Spin 8.5<br>阿岳正传 8.3<br>感官/世界 8.7<br>八度空间 7.5<br>PCD 8.4<br>…</p>
</blockquote>
</div><footer class="post__foot u-cf"><ul class="post__tag u-fl"><li class="post__tag__item"><a href="/tags/Python/" class="post__tag__link">Python</a></li></ul><a href="/2014/Python/douban-crawler/#disqus_thread" class="post__foot-link u-fr">0 COMMENTS</a></footer></article></main><footer class="foot"><div class="foot-copy u-fl">&copy; 2019 thinKnight</div><menu class="page-menu u-fr"><li class="page-menu__item"><span title="Previous" class="page-menu__link icon-arrow-left page-menu__link--disabled"></span></li><li class="page-menu__item"><span title="Next" class="page-menu__link icon-arrow-right page-menu__link--disabled"></span></li></menu></footer><script>(function(h,g,l,k,j,i){j=h.createElement(g),i=h.getElementsByTagName(g)[0],
j.src="//"+l+".disqus.com/"+k+".js",i.parentNode.insertBefore(j,i)})
(document,"script","matrixwall","count");
</script></body></html>